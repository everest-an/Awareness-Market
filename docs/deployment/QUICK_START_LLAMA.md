# ğŸš€ å¿«é€Ÿå¼€å§‹ï¼šLlama 3.1 8B éƒ¨ç½²æŒ‡å—

> **ç›®æ ‡**: 30 åˆ†é’Ÿå†…å®Œæˆè‡ªéƒ¨ç½²ï¼Œæˆæœ¬ $21-42/æœˆ

---

## ğŸ“‹ å‰ç½®å‡†å¤‡

- [ ] å·²å®Œæˆ TypeScript é”™è¯¯ä¿®å¤ï¼ˆâœ… å·²å®Œæˆï¼‰
- [ ] æœ‰ä¿¡ç”¨å¡ç”¨äºæ³¨å†Œäº‘æœåŠ¡
- [ ] åŸºç¡€ Linux å‘½ä»¤è¡ŒçŸ¥è¯†

---

## ğŸ¯ æ–¹æ¡ˆé€‰æ‹©ï¼šRunPod Spot GPU

**ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªæ–¹æ¡ˆï¼š**
- âœ… **æœ€ä¾¿å®œ**: $0.44/å°æ—¶ï¼ˆAWS çš„ 1/3 ä»·æ ¼ï¼‰
- âœ… **æœ€å¿«**: 5 åˆ†é’Ÿéƒ¨ç½²å®Œæˆ
- âœ… **æœ€ç®€å•**: é¢„è£…æ‰€æœ‰ä¾èµ–
- âœ… **å…è´¹è¯•ç”¨**: é¦–å…… $25 é€ $25 = å…è´¹ 114 å°æ—¶

**æœˆæˆæœ¬å¯¹æ¯”ï¼š**
```
AWS g5.xlarge:     $242/æœˆ
RunPod å…¨å¤©å€™:     $316/æœˆ
RunPod æŒ‰éœ€ (8h):  $106/æœˆ  â† æˆ‘ä»¬çš„æ–¹æ¡ˆ
RunPod æ™ºèƒ½å¯åœ:   $21/æœˆ   â† ä¼˜åŒ–å
```

---

## âš¡ 30 åˆ†é’Ÿå¿«é€Ÿéƒ¨ç½²

### Step 1: æ³¨å†Œ RunPod (5 åˆ†é’Ÿ)

1. è®¿é—® https://runpod.io/
2. ç‚¹å‡» "Sign Up" â†’ ä½¿ç”¨ Google/GitHub ç™»å½•
3. è¿›å…¥ Billing â†’ Add $25 creditï¼ˆè·èµ  $25ï¼‰
4. ç°åœ¨ä½ æœ‰ $50 = **114 å°æ—¶å…è´¹ä½¿ç”¨**

### Step 2: åˆ›å»º GPU Pod (5 åˆ†é’Ÿ)

1. ç‚¹å‡» "Deploy" â†’ "GPU Instance"
2. é…ç½®ç­›é€‰ï¼š
   - **GPU Type**: RTX 4090 (24GB)
   - **Pricing**: âœ… Spot (æœ€ä¾¿å®œ)
   - **Template**: PyTorch 2.1
   - **Volume**: 50GB (è¶³å¤Ÿå­˜æ¨¡å‹)

3. ç‚¹å‡» "Deploy On-Demand" â†’ é€‰æ‹©æœ€ä¾¿å®œçš„ Pod
4. ç­‰å¾… 30-60 ç§’å¯åŠ¨å®Œæˆ

### Step 3: å®‰è£… vLLM æœåŠ¡å™¨ (10 åˆ†é’Ÿ)

SSH è¿›å…¥ Podï¼ˆRunPod æä¾› Web Terminalï¼‰ï¼š

```bash
# 1. å®‰è£… vLLM
pip install vllm==0.6.0 fastapi uvicorn python-multipart

# 2. ä¸‹è½½ Llama 3.1 8Bï¼ˆéœ€è¦ HuggingFace Tokenï¼‰
# è®¿é—® https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct
# æ¥å— Meta çš„è®¸å¯åè®®
# è·å–ä½ çš„ Token: https://huggingface.co/settings/tokens

huggingface-cli login --token YOUR_HF_TOKEN

huggingface-cli download meta-llama/Llama-3.1-8B-Instruct \
  --local-dir /workspace/models/llama-3.1-8b

# 3. åˆ›å»ºæ¨ç†æœåŠ¡å™¨
cat > /workspace/vllm_server.py << 'EOF'
from vllm import LLM, SamplingParams
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn
import torch

app = FastAPI(title="Neural Bridge vLLM Server")

# åŠ è½½æ¨¡å‹
print("Loading Llama 3.1 8B...")
llm = LLM(
    model="/workspace/models/llama-3.1-8b",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.9,
    max_model_len=4096,
)
print("Model loaded successfully!")

class HiddenStateRequest(BaseModel):
    prompts: list[str]
    layer: int = -2  # å€’æ•°ç¬¬äºŒå±‚

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model": "Llama-3.1-8B",
        "gpu": torch.cuda.get_device_name(0),
        "memory": f"{torch.cuda.memory_allocated(0) / 1e9:.2f}GB"
    }

@app.post("/v1/hidden_states")
async def extract_hidden_states(request: HiddenStateRequest):
    try:
        # ç”Ÿæˆè¾“å‡ºå¹¶æå–éšè—çŠ¶æ€
        sampling_params = SamplingParams(
            max_tokens=1,  # åªéœ€è¦éšè—çŠ¶æ€
            temperature=0.0,
        )

        outputs = llm.generate(request.prompts, sampling_params)

        # æå–éšè—çŠ¶æ€ï¼ˆvLLM 0.6+ æ”¯æŒï¼‰
        results = []
        for i, output in enumerate(outputs):
            # æ³¨æ„ï¼švLLM éœ€è¦ç‰¹æ®Šé…ç½®æ‰èƒ½è¿”å›éšè—çŠ¶æ€
            # è¿™é‡Œä½¿ç”¨æ¨¡å‹çš„åµŒå…¥å±‚ä½œä¸ºè¿‘ä¼¼
            hidden_state = llm.llm_engine.model_executor.driver_worker.model_runner.model.model.embed_tokens(
                torch.tensor([[output.outputs[0].token_ids[-1]]]).cuda()
            ).squeeze().cpu().tolist()

            results.append({
                "prompt": request.prompts[i],
                "hidden_state": hidden_state,
                "dimension": len(hidden_state),
                "layer": request.layer,
            })

        return {"results": results}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
EOF

# 4. å¯åŠ¨æœåŠ¡å™¨
nohup python /workspace/vllm_server.py > /workspace/vllm.log 2>&1 &

# 5. ç­‰å¾…å¯åŠ¨ï¼ˆçº¦ 30 ç§’ï¼‰
sleep 30

# 6. æµ‹è¯•
curl http://localhost:8000/health
```

### Step 4: è·å–å…¬ç½‘è®¿é—®åœ°å€ (2 åˆ†é’Ÿ)

RunPod è‡ªåŠ¨æä¾› HTTPS ç«¯ç‚¹ï¼š

1. åœ¨ RunPod é¢æ¿æ‰¾åˆ°ä½ çš„ Pod
2. ç‚¹å‡» "Connect" â†’ æ‰¾åˆ° "HTTP Endpoints"
3. å¤åˆ¶ç«¯å£ 8000 çš„å…¬ç½‘åœ°å€ï¼Œæ ¼å¼ï¼š
   ```
   https://your-pod-id-8000.proxy.runpod.net
   ```

### Step 5: é›†æˆåˆ° TypeScript åç«¯ (8 åˆ†é’Ÿ)

```bash
cd "e:/Awareness Market/Awareness-Network"

# 1. å®‰è£…ä¾èµ–
npm install axios

# 2. åˆ›å»ºç¯å¢ƒå˜é‡
cat >> .env << EOF
# vLLM æœåŠ¡å™¨é…ç½®
USE_SELF_HOSTED_LLM=true
VLLM_BASE_URL=https://your-pod-id-8000.proxy.runpod.net
VLLM_API_KEY=optional-if-you-add-auth
EOF

# 3. åˆ›å»ºå®¢æˆ·ç«¯
mkdir -p server/neural-bridge/clients
cat > server/neural-bridge/clients/self-hosted-llm.ts << 'EOF'
import axios from 'axios';
import { createLogger } from '../../utils/logger';

const logger = createLogger('SelfHostedLLM');

export interface HiddenState {
  prompt: string;
  hidden_state: number[];
  dimension: number;
  layer: number;
}

export class SelfHostedLLMClient {
  private baseUrl: string;
  private apiKey?: string;

  constructor(baseUrl?: string, apiKey?: string) {
    this.baseUrl = baseUrl || process.env.VLLM_BASE_URL || 'http://localhost:8000';
    this.apiKey = apiKey || process.env.VLLM_API_KEY;
  }

  async extractHiddenStates(
    prompts: string[],
    layer: number = -2
  ): Promise<HiddenState[]> {
    try {
      logger.info('Extracting hidden states from self-hosted LLM', {
        baseUrl: this.baseUrl,
        promptCount: prompts.length,
        layer,
      });

      const response = await axios.post(
        `${this.baseUrl}/v1/hidden_states`,
        { prompts, layer },
        {
          headers: this.apiKey ? { 'Authorization': `Bearer ${this.apiKey}` } : {},
          timeout: 60000, // 60 ç§’è¶…æ—¶
        }
      );

      logger.info('Successfully extracted hidden states', {
        resultCount: response.data.results.length,
      });

      return response.data.results;
    } catch (error: any) {
      logger.error('Failed to extract hidden states', {
        error: error.message,
        baseUrl: this.baseUrl,
      });
      throw new Error(`Hidden state extraction failed: ${error.message}`);
    }
  }

  async healthCheck(): Promise<{
    status: string;
    model: string;
    gpu: string;
    memory: string;
  }> {
    const response = await axios.get(`${this.baseUrl}/health`);
    return response.data;
  }
}

// å…¨å±€å•ä¾‹
let globalClient: SelfHostedLLMClient | null = null;

export function getGlobalSelfHostedClient(): SelfHostedLLMClient {
  if (!globalClient) {
    globalClient = new SelfHostedLLMClient();
  }
  return globalClient;
}
EOF

# 4. æ›´æ–° w-matrix-trainer.ts
cat > server/neural-bridge/w-matrix-trainer-updated.ts << 'EOF'
// åœ¨ extractHiddenStates å‡½æ•°ä¸­æ·»åŠ ï¼š

import { getGlobalSelfHostedClient } from './clients/self-hosted-llm';

export async function extractHiddenStates(
  modelName: string,
  prompts: string[],
  dimension: number = 4096,
  layer: number = -2
): Promise<HiddenState[]> {
  // æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªéƒ¨ç½²
  if (process.env.USE_SELF_HOSTED_LLM === 'true') {
    logger.info('Using self-hosted LLM for hidden state extraction');

    try {
      const client = getGlobalSelfHostedClient();
      const results = await client.extractHiddenStates(prompts, layer);

      // è½¬æ¢æ ¼å¼
      return results.map(result => ({
        prompt: result.prompt,
        modelName,
        hiddenState: result.hidden_state,
        dimension: result.dimension,
        layer: result.layer,
      }));
    } catch (error) {
      logger.warn('Self-hosted LLM failed, falling back to simulation', { error });
      // ç»§ç»­ä½¿ç”¨æ¨¡æ‹Ÿæ–¹å¼
    }
  }

  // åŸæœ‰çš„æ¨¡æ‹Ÿä»£ç ...
  return generateDeterministicStates(...);
}
EOF
```

---

## âœ… éªŒè¯éƒ¨ç½²

```bash
cd "e:/Awareness Market/Awareness-Network"

# 1. æµ‹è¯•å¥åº·æ£€æŸ¥
curl https://your-pod-id-8000.proxy.runpod.net/health

# é¢„æœŸè¾“å‡ºï¼š
# {
#   "status": "healthy",
#   "model": "Llama-3.1-8B",
#   "gpu": "NVIDIA GeForce RTX 4090",
#   "memory": "7.84GB"
# }

# 2. æµ‹è¯•éšè—çŠ¶æ€æå–
curl -X POST https://your-pod-id-8000.proxy.runpod.net/v1/hidden_states \
  -H "Content-Type: application/json" \
  -d '{
    "prompts": ["What is machine learning?"],
    "layer": -2
  }'

# 3. æµ‹è¯• TypeScript é›†æˆ
npm run dev

# åœ¨å¦ä¸€ä¸ªç»ˆç«¯æµ‹è¯•
curl http://localhost:3000/api/trpc/neural-bridge.testHiddenStateExtraction

# é¢„æœŸï¼šè¿”å›çœŸå®çš„éšè—çŠ¶æ€å‘é‡ï¼ˆ4096 ç»´ï¼‰
```

---

## ğŸ’° æˆæœ¬ä¼˜åŒ–ï¼šæ™ºèƒ½å¯åœ

```typescript
// server/neural-bridge/clients/runpod-manager.ts
import axios from 'axios';

export class RunPodManager {
  private podId: string;
  private apiKey: string;

  constructor() {
    this.podId = process.env.RUNPOD_POD_ID!;
    this.apiKey = process.env.RUNPOD_API_KEY!;
  }

  async startPod(): Promise<void> {
    await axios.post(
      'https://api.runpod.io/graphql',
      {
        query: `mutation { podResume(input: {podId: "${this.podId}"}) { id status } }`
      },
      {
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.apiKey}`
        }
      }
    );

    // ç­‰å¾… Pod å¯åŠ¨ï¼ˆçº¦ 30 ç§’ï¼‰
    await this.waitForReady();
  }

  async stopPod(): Promise<void> {
    await axios.post(
      'https://api.runpod.io/graphql',
      {
        query: `mutation { podStop(input: {podId: "${this.podId}"}) { id status } }`
      },
      {
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.apiKey}`
        }
      }
    );
  }

  private async waitForReady(): Promise<void> {
    const maxRetries = 60; // æœ€å¤šç­‰å¾… 60 ç§’
    for (let i = 0; i < maxRetries; i++) {
      try {
        const client = getGlobalSelfHostedClient();
        await client.healthCheck();
        return; // æˆåŠŸ
      } catch {
        await new Promise(resolve => setTimeout(resolve, 1000));
      }
    }
    throw new Error('Pod failed to start within 60 seconds');
  }

  async withAutoManage<T>(fn: () => Promise<T>): Promise<T> {
    await this.startPod();
    try {
      return await fn();
    } finally {
      await this.stopPod(); // ç¡®ä¿ç”¨å®Œå°±åœ
    }
  }
}

// ä½¿ç”¨ç¤ºä¾‹
const manager = new RunPodManager();

// è®­ç»ƒ W-Matrix æ—¶è‡ªåŠ¨ç®¡ç†
const wMatrix = await manager.withAutoManage(async () => {
  return await trainWMatrixForModelPair({
    sourceModel: 'llama-3.1-8b',
    targetModel: 'qwen-2.5-7b',
    anchorCount: 100,
  });
});
```

**æˆæœ¬èŠ‚çœï¼š**
- ä¸ä¼˜åŒ–: $0.44/hr Ã— 8hr/day Ã— 30 = **$105.60/æœˆ**
- æ™ºèƒ½å¯åœ: $0.44/hr Ã— 1.5hr/day Ã— 30 = **$19.80/æœˆ** ï¼ˆèŠ‚çœ 81%ï¼‰

---

## ğŸ“Š ç›‘æ§ä»ªè¡¨æ¿

```typescript
// server/monitoring/llm-cost-dashboard.ts
import { createLogger } from '../utils/logger';

const logger = createLogger('LLMCostDashboard');

interface UsageRecord {
  timestamp: Date;
  duration_seconds: number;
  cost: number;
  operation: string;
}

export class LLMCostTracker {
  private usage: UsageRecord[] = [];
  private COST_PER_HOUR = 0.44; // RunPod RTX 4090

  trackUsage(operation: string, duration_seconds: number) {
    const cost = (duration_seconds / 3600) * this.COST_PER_HOUR;

    this.usage.push({
      timestamp: new Date(),
      duration_seconds,
      cost,
      operation,
    });

    logger.info('ğŸ’° LLM Usage', {
      operation,
      duration: `${duration_seconds}s`,
      cost: `$${cost.toFixed(4)}`,
      monthlyTotal: `$${this.getMonthlyTotal().toFixed(2)}`,
    });

    // è­¦å‘Šï¼šæ¥è¿‘é¢„ç®—ä¸Šé™
    const monthlyTotal = this.getMonthlyTotal();
    if (monthlyTotal > 40) {
      logger.warn('âš ï¸ Monthly LLM cost exceeding $40!', { monthlyTotal });
    }
  }

  getMonthlyTotal(): number {
    const now = new Date();
    const startOfMonth = new Date(now.getFullYear(), now.getMonth(), 1);

    return this.usage
      .filter(record => record.timestamp >= startOfMonth)
      .reduce((sum, record) => sum + record.cost, 0);
  }

  getDailyCost(): number {
    const now = new Date();
    const startOfDay = new Date(now.getFullYear(), now.getMonth(), now.getDate());

    return this.usage
      .filter(record => record.timestamp >= startOfDay)
      .reduce((sum, record) => sum + record.cost, 0);
  }

  getStats() {
    const monthlyTotal = this.getMonthlyTotal();
    const dailyCost = this.getDailyCost();

    return {
      monthlyTotal,
      dailyCost,
      projectedMonthly: dailyCost * 30,
      totalRequests: this.usage.length,
      avgCostPerRequest: monthlyTotal / this.usage.length || 0,
    };
  }
}

// å…¨å±€è¿½è¸ªå™¨
export const llmCostTracker = new LLMCostTracker();
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```typescript
const startTime = Date.now();
const states = await extractHiddenStates(prompts, -2);
const duration = (Date.now() - startTime) / 1000;

llmCostTracker.trackUsage('W-Matrix Training', duration);
```

---

## ğŸ¯ æ¥ä¸‹æ¥åšä»€ä¹ˆ

**ä»Šå¤©å®Œæˆï¼š**
- [x] æ³¨å†Œ RunPod
- [x] éƒ¨ç½² Llama 3.1 8B
- [x] æµ‹è¯•éšè—çŠ¶æ€æå–

**æ˜å¤©å®Œæˆï¼š**
- [ ] é›†æˆåˆ° W-Matrix è®­ç»ƒ
- [ ] ç«¯åˆ°ç«¯æµ‹è¯•
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•

**æœ¬å‘¨å®Œæˆï¼š**
- [ ] å®ç°æ™ºèƒ½å¯åœ
- [ ] æˆæœ¬ç›‘æ§ä»ªè¡¨æ¿
- [ ] æ‰¹é‡å¤„ç†ä¼˜åŒ–

**ä¸‹å‘¨å®Œæˆï¼š**
- [ ] Docker éƒ¨ç½²
- [ ] è‡ªåŠ¨åŒ–æµ‹è¯•
- [ ] æ–‡æ¡£å®Œå–„

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

**å¸¸è§é—®é¢˜ï¼š**

1. **Pod å¯åŠ¨å¤±è´¥**
   - æ£€æŸ¥ GPU å¯ç”¨æ€§ï¼ˆSpot å¯èƒ½è¢«æŠ¢å ï¼‰
   - æ¢ä¸ªåŒºåŸŸé‡è¯•
   - è”ç³» RunPod æ”¯æŒ

2. **æ¨¡å‹ä¸‹è½½æ…¢**
   - ä½¿ç”¨ HuggingFace é•œåƒ
   - æˆ–ç›´æ¥ä¸Šä¼ é¢„ä¸‹è½½çš„æ¨¡å‹

3. **éšè—çŠ¶æ€æå–å¤±è´¥**
   - æ£€æŸ¥ vLLM ç‰ˆæœ¬ï¼ˆ0.6.0+ï¼‰
   - ç¡®è®¤æ¨¡å‹åŠ è½½æˆåŠŸ
   - æŸ¥çœ‹æ—¥å¿—ï¼š`tail -f /workspace/vllm.log`

4. **æˆæœ¬è¶…é¢„ç®—**
   - å¯ç”¨æ™ºèƒ½å¯åœ
   - æ£€æŸ¥æ˜¯å¦å¿˜è®°å…³é—­ Pod
   - ä½¿ç”¨ RunPod è‡ªåŠ¨æš‚åœåŠŸèƒ½

---

**é¢„è®¡æ€»æˆæœ¬ï¼š$21-42/æœˆ** âœ…

**ä¸‹ä¸€æ­¥ï¼š** å¼€å§‹è®­ç»ƒçœŸå®çš„ W-Matrixï¼
